# IVIE - åŒºé—´å€¼æ¨¡ç³Šç§¯åˆ†ç¥ç»ç½‘ç»œ

## æ¦‚è¿°

IVIE (Interval-Valued Integral Entropy) æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„åŒºé—´å€¼æ¨¡ç³Šç§¯åˆ†ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚è¯¥ç½‘ç»œä¸“é—¨ç”¨äºå¤„ç†åŒºé—´å€¼æ•°æ®ï¼Œé€šè¿‡æ¨¡ç³Šæµ‹åº¦å’Œ IVIE ç§¯åˆ†å®ç°å¯¹ä¸ç¡®å®šæ€§æ•°æ®çš„å»ºæ¨¡å’Œé¢„æµ‹ã€‚

## æœ€æ–°æ›´æ–° (2026å¹´1æœˆ)

### ğŸš€ å…³é”®æ”¹è¿›

1. **æ”¯æŒé™åˆ¶äº¤äº’é˜¶æ•° (additivity_order)**
   - é¿å…é«˜é˜¶ç‰¹å¾ç»„åˆå¯¼è‡´çš„æ•°å€¼ä¸‹æº¢é—®é¢˜
   - ç‰¹åˆ«é€‚ç”¨äº `Algebraic_interval` æ“ä½œ
   - æ˜¾è‘—æå‡æ¨¡å‹åœ¨å®é™…æ•°æ®é›†ä¸Šçš„æ€§èƒ½

2. **æ–°å¢æ”¹è¿›çš„æŸå¤±å‡½æ•°**
   - `ImprovedIntervalLoss`: åŒ…å«ç«¯ç‚¹MSEã€åŒºé—´æœ‰æ•ˆæ€§æƒ©ç½šã€å®½åº¦åŒ¹é…
   - `HausdorffIntervalLoss`: åŸºäºHausdorffè·ç¦»çš„æŸå¤±å‡½æ•°
   - æ›´å¥½åœ°çº¦æŸåŒºé—´é¢„æµ‹çš„åˆç†æ€§

3. **ä¼˜åŒ–çš„è®­ç»ƒç­–ç•¥**
   - å­¦ä¹ ç‡è°ƒåº¦ (CosineAnnealingLR)
   - æ—©åœæœºåˆ¶ (Early Stopping)
   - æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
   - AdamWä¼˜åŒ–å™¨é…åˆæƒé‡è¡°å‡

## é¡¹ç›®ç»“æ„

```
IVIE/
â”œâ”€â”€ __init__.py          # æ¨¡å—åˆå§‹åŒ–æ–‡ä»¶
â”œâ”€â”€ ivie.py              # IE ç½‘ç»œä¸»ç±» (æ”¯æŒ additivity_order)
â”œâ”€â”€ iv_loss.py           # æŸå¤±å‡½æ•°æ¨¡å— (æ–°å¢æ”¹è¿›çš„æŸå¤±å‡½æ•°)
â”œâ”€â”€ narray_op.py         # åŒºé—´è¿ç®—æ“ä½œæ¨¡å—
â”œâ”€â”€ feature_layer.py     # ç‰¹å¾çŸ©é˜µæ„å»ºæ¨¡å—
â””â”€â”€ README.md            # æœ¬æ–‡æ¡£
```

---

## æ ¸å¿ƒæ¨¡å—ä»‹ç»

### 1. IE ç½‘ç»œä¸»ç±» (`ivie.py`)

`IE` ç±»æ˜¯æ•´ä¸ªæ¡†æ¶çš„æ ¸å¿ƒï¼Œç»§æ‰¿è‡ª `torch.nn.Module`ï¼Œå®ç°äº†åŒºé—´å€¼æ¨¡ç³Šç§¯åˆ†ç¥ç»ç½‘ç»œã€‚

#### ç±»å®šä¹‰

```python
class IE(nn.Module):
    def __init__(self, feature_size, additivity_order=None, 
                 op='Algebraic_interval', alpha=1, beta=0, device='cuda')
```

#### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `feature_size` | int | å¿…å¡« | è¾“å…¥ç‰¹å¾çš„æ•°é‡ |
| `additivity_order` | int | None | **å¯åŠ æ€§é˜¶æ•°**ï¼Œæ§åˆ¶ç‰¹å¾äº¤äº’çš„æœ€å¤§é˜¶æ•°ã€‚è‹¥ä¸º Noneï¼Œåˆ™ç­‰äº feature_sizeã€‚<br>âš ï¸ **é‡è¦**: ä½¿ç”¨ `Algebraic_interval` æ—¶å»ºè®®è®¾ç½®ä¸º 2-3ï¼Œé¿å…æ•°å€¼ä¸‹æº¢ |
| `op` | str | 'Algebraic_interval' | åŒºé—´è¿ç®—ç±»å‹ï¼Œå¯é€‰ `'Algebraic_interval'` æˆ– `'Min_interval'` |
| `alpha` | float | 1 | Min_interval æ“ä½œçš„ alpha å‚æ•°ï¼Œç”¨äºåŒºé—´æ¯”è¾ƒ |
| `beta` | float | 0 | Min_interval æ“ä½œçš„ beta å‚æ•°ï¼Œç”¨äºå¹³å±€æ—¶çš„å†³ç­– |
| `device` | str | 'cuda' | è®¡ç®—è®¾å¤‡ ('cuda' æˆ– 'cpu') |

#### âš ï¸ é‡è¦æç¤ºï¼šadditivity_order å‚æ•°

å½“ä½¿ç”¨ `Algebraic_interval` æ“ä½œæ—¶ï¼Œé«˜é˜¶ç‰¹å¾ç»„åˆä¼šé€šè¿‡è¿ç»­ä¹˜æ³•ç”Ÿæˆã€‚å¯¹äºå½’ä¸€åŒ–åˆ° [0,1] çš„æ•°æ®ï¼š

- **é—®é¢˜**: 7ä¸ªç‰¹å¾ Ã— æ‰€æœ‰é˜¶æ•° â†’ 127ä¸ªç»„åˆï¼Œ7é˜¶ç»„åˆçº¦ä¸º 0.3^7 â‰ˆ 2Ã—10â»â´ï¼Œå¯¼è‡´æ•°å€¼ä¸‹æº¢
- **è§£å†³æ–¹æ¡ˆ**: è®¾ç½® `additivity_order=2` æˆ– `3`ï¼Œåªè€ƒè™‘ä½é˜¶äº¤äº’
- **æ•ˆæœ**: 
  - `additivity_order=2`: ç”Ÿæˆ C(n,1) + C(n,2) ä¸ªç‰¹å¾ (ä¾‹å¦‚7ä¸ªç‰¹å¾â†’28ä¸ªç»„åˆ)
  - é¿å…æ•°å€¼é—®é¢˜çš„åŒæ—¶ä¿ç•™ä¸»è¦äº¤äº’ä¿¡æ¯

```python
# âŒ ä¸æ¨èï¼šä¼šå¯¼è‡´æ•°å€¼ä¸‹æº¢
model = IE(feature_size=7, op='Algebraic_interval')  # ç”Ÿæˆ127ä¸ªç‰¹å¾ï¼Œé«˜é˜¶ç‰¹å¾è¶‹è¿‘äº0

# âœ… æ¨èï¼šé™åˆ¶äº¤äº’é˜¶æ•°
model = IE(feature_size=7, additivity_order=2, op='Algebraic_interval')  # ç”Ÿæˆ28ä¸ªç‰¹å¾
```

#### ç½‘ç»œç»“æ„

```
è¾“å…¥å±‚ (åŒºé—´å€¼æ•°æ®)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     è¾“å…¥è§£æ                         â”‚
â”‚  x = [x_l, x_u]                     â”‚
â”‚  åˆ†ç¦»å·¦ç«¯ç‚¹ datal å’Œå³ç«¯ç‚¹ datau      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     åŒºé—´è¿ç®—å±‚ (narray_op)           â”‚
â”‚  - Algebraic_interval: åŒºé—´ä¹˜æ³•      â”‚
â”‚  - Min_interval: åŒºé—´æœ€å°å€¼é€‰æ‹©      â”‚
â”‚  ç”Ÿæˆæ‰€æœ‰ç‰¹å¾ç»„åˆçš„åŒºé—´å€¼             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ç‰¹å¾çŸ©é˜µå±‚ (feature_matrix)      â”‚
â”‚  ç¨€ç– 01 çŸ©é˜µå˜æ¢                    â”‚
â”‚  å½¢çŠ¶: (2^n-1, 2*(2^n-1))           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     æ¨¡ç³Šæµ‹åº¦å±‚ (FM)                  â”‚
â”‚  å¯å­¦ä¹ å‚æ•° vars: (2^n-2, 1)        â”‚
â”‚  é€šè¿‡ ivie_nn_vars è½¬æ¢ä¸º FM         â”‚
â”‚  ä¿è¯å•è°ƒæ€§çº¦æŸ                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     IVIE ç§¯åˆ†è®¡ç®—                    â”‚
â”‚  åŒºé—´å‡æ³•: left = min(a-c,b-d)       â”‚
â”‚           right = b - d             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
è¾“å‡ºå±‚ (é¢„æµ‹åŒºé—´ [left, right])
```

#### å…³é”®æ–¹æ³•

##### `forward(x)`
å‰å‘ä¼ æ’­æ–¹æ³•ï¼Œæ¥æ”¶åŒºé—´å€¼è¾“å…¥å¹¶è¿”å›é¢„æµ‹åŒºé—´ã€‚

- **è¾“å…¥**: `x` - å½¢çŠ¶ä¸º `(batch, 2*n_features)` çš„å¼ é‡ï¼Œå‰åŠéƒ¨åˆ†æ˜¯å·¦ç«¯ç‚¹ï¼ŒååŠéƒ¨åˆ†æ˜¯å³ç«¯ç‚¹
- **è¾“å‡º**: `(left, right)` - é¢„æµ‹åŒºé—´çš„å·¦ç«¯ç‚¹å’Œå³ç«¯ç‚¹ï¼Œå½¢çŠ¶å‡ä¸º `(batch, 1)`

##### `ivie_nn_vars(ivie_vars)`
å°†ç¥ç»ç½‘ç»œå‚æ•°è½¬æ¢ä¸ºæ»¡è¶³å•è°ƒæ€§çº¦æŸçš„æ¨¡ç³Šæµ‹åº¦ (Fuzzy Measure)ã€‚

- ç¡®ä¿ FM å€¼éè´Ÿï¼ˆé€šè¿‡å–ç»å¯¹å€¼ï¼‰
- ç¡®ä¿å•è°ƒæ€§ï¼šå¯¹äºå­é›†å…³ç³» $A \subseteq B$ï¼Œæœ‰ $\mu(A) \leq \mu(B)$
- å½’ä¸€åŒ–ï¼š$\mu(\emptyset) = 0$ï¼Œ$\mu(X) = 1$

##### `fit_and_valid(train_Loader, test_Loader, criterion, optimizer, device, epochs)`
è®­ç»ƒå’ŒéªŒè¯æ–¹æ³•ã€‚

- **å‚æ•°**:
  - `train_Loader`: è®­ç»ƒæ•°æ®åŠ è½½å™¨
  - `test_Loader`: æµ‹è¯•æ•°æ®åŠ è½½å™¨
  - `criterion`: æŸå¤±å‡½æ•°
  - `optimizer`: ä¼˜åŒ–å™¨
  - `device`: è®¡ç®—è®¾å¤‡
  - `epochs`: è®­ç»ƒè½®æ•°

---

### 2. åŒºé—´è¿ç®—æ¨¡å— (`narray_op.py`)

è¯¥æ¨¡å—å®ç°äº†ä¸¤ç§åŒºé—´è¿ç®—æ“ä½œï¼Œç”¨äºè®¡ç®—ç‰¹å¾çš„æ‰€æœ‰å¯èƒ½ç»„åˆã€‚

#### 2.1 Algebraic_interval ç±»

**åŒºé—´ä»£æ•°ä¹˜æ³•**è¿ç®—ï¼Œç”¨äºè®¡ç®—ç‰¹å¾ç»„åˆçš„ä¹˜ç§¯ã€‚

$$[a, b] \times [c, d] = [a \cdot c, b \cdot d]$$

```python
class Algebraic_interval(nn.Module):
    def __init__(self, add)
    def forward(self, xl, xu) -> (nodes_tnorml, nodes_tnormu)
```

- **add**: å¯åŠ æ€§é˜¶æ•°ï¼Œæ§åˆ¶ç»„åˆçš„æœ€å¤§é•¿åº¦
- **è¾“å…¥**: `xl` (å·¦ç«¯ç‚¹), `xu` (å³ç«¯ç‚¹)ï¼Œå½¢çŠ¶ä¸º `(batch, n_features)`
- **è¾“å‡º**: æ‰€æœ‰ç»„åˆçš„åŒºé—´å€¼ï¼ŒæŒ‰ä½ç¼–ç é¡ºåºæ’åˆ—

#### 2.2 Min_interval ç±»

**åŒºé—´æœ€å°å€¼é€‰æ‹©**è¿ç®—ï¼ŒåŸºäº alpha-beta å‚æ•°é€‰æ‹©è¾ƒå°çš„åŒºé—´ã€‚

```python
class Min_interval(nn.Module):
    def __init__(self, add, alpha, beta)
    def forward(self, xl, xu) -> (nodes_tnorml, nodes_tnormu)
```

**é€‰æ‹©è§„åˆ™**:
1. è®¡ç®—ä»£è¡¨å€¼ï¼š$v = (1-\alpha) \cdot l + \alpha \cdot u$
2. é€‰æ‹©ä»£è¡¨å€¼è¾ƒå°çš„åŒºé—´
3. è‹¥ç›¸ç­‰ï¼Œä½¿ç”¨ beta å‚æ•°è¿›è¡Œå†³ç­–

---

### 3. ç‰¹å¾çŸ©é˜µæ¨¡å— (`feature_layer.py`)

#### FeatureMatrix ç±»

æ„å»ºç”¨äº Choquet ç§¯åˆ†è®¡ç®—çš„ç¨€ç– 01 çŸ©é˜µã€‚

```python
class FeatureMatrix:
    def __init__(self, n: int, device: str = 'cpu')
    def build_sparse_matrix(self) -> torch.Tensor
```

**æ•°å­¦åŸç†**:
- è¶…é›†è¡¨ç¤º: $T = S \cup E$, å…¶ä¸­ $E \subseteq \bar{S}$
- å·®é›†å¤§å°: $|T \setminus S| = |E| = \text{popcount}(e)$
- å­é›†æšä¸¾: $e_{k+1} = (e_k - 1) \land \text{complement}$

**çŸ©é˜µå±æ€§**:
- å½¢çŠ¶: $(2^n - 1, 2 \times (2^n - 1))$
- ä½¿ç”¨ç¨€ç– COO æ ¼å¼å­˜å‚¨
- éé›¶å…ƒç´ æ•°é‡çº¦ä¸º $3^n - 2^n$

---

## æŸå¤±å‡½æ•°æ¨¡å— (`iv_loss.py`)

æ¨¡å—æä¾›äº†ä¸‰ç§æŸå¤±å‡½æ•°ï¼Œé€‚ç”¨äºä¸åŒçš„è®­ç»ƒéœ€æ±‚ã€‚

### 1. interval_loss (åŸå§‹æŸå¤±å‡½æ•°)

åŸºäºHausdorffè·ç¦»çš„ç®€å•æŸå¤±å‡½æ•°ã€‚

```python
class interval_loss(nn.Module):
    def forward(self, rel, reu, ta) -> (loss, distance)
```

**è®¡ç®—å…¬å¼**:
$$\text{loss} = \mathbb{E}\left[\left(\frac{1}{2}\sqrt{(r_l - t_l)^2 + (r_u - t_u)^2}\right)^2\right]$$

### 2. ImprovedIntervalLoss (æ¨è)

æ”¹è¿›çš„æŸå¤±å‡½æ•°ï¼ŒåŒ…å«å¤šä¸ªçº¦æŸé¡¹ã€‚

```python
class ImprovedIntervalLoss(nn.Module):
    def __init__(self, validity_weight=0.1, width_weight=0.05)
    def forward(self, rel, reu, ta) -> (total_loss, distance)
```

**æŸå¤±ç»„æˆ**:
1. **ç«¯ç‚¹MSEæŸå¤±**: $L_{MSE} = \mathbb{E}[(r_l - t_l)^2] + \mathbb{E}[(r_u - t_u)^2]$
2. **åŒºé—´æœ‰æ•ˆæ€§æŸå¤±**: $L_{valid} = \mathbb{E}[\max(0, r_l - r_u)]$ (æƒ©ç½šæ— æ•ˆåŒºé—´)
3. **å®½åº¦åŒ¹é…æŸå¤±**: $L_{width} = \mathbb{E}[((r_u - r_l) - (t_u - t_l))^2]$

**æ€»æŸå¤±**:
$$L_{total} = L_{MSE} + w_v \cdot L_{valid} + w_w \cdot L_{width}$$

**ä¼˜åŠ¿**:
- âœ… ç¡®ä¿é¢„æµ‹åŒºé—´æœ‰æ•ˆæ€§ (ä¸‹ç•Œ â‰¤ ä¸Šç•Œ)
- âœ… åŒ¹é…åŒºé—´å®½åº¦ï¼Œé¿å…è¿‡å¤§æˆ–è¿‡å°çš„é¢„æµ‹
- âœ… æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§

### 3. HausdorffIntervalLoss

åŸºäºHausdorffè·ç¦»çš„æ”¹è¿›ç‰ˆæœ¬ã€‚

```python
class HausdorffIntervalLoss(nn.Module):
    def __init__(self, validity_weight=0.1)
    def forward(self, rel, reu, ta) -> (total_loss, hausdorff)
```

**è®¡ç®—å…¬å¼**:
$$d_H([r_l, r_u], [t_l, t_u]) = \max(|r_l - t_l|, |r_u - t_u|)$$

---


## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
import torch
from IVIE.ivie import IE

# åˆ›å»ºæ¨¡å‹ (æ¨èé…ç½®)
model = IE(
    feature_size=7,            # 7 ä¸ªç‰¹å¾
    additivity_order=2,        # åªè€ƒè™‘2é˜¶äº¤äº’ï¼Œé¿å…æ•°å€¼é—®é¢˜
    op='Algebraic_interval',   # ä½¿ç”¨ä»£æ•°åŒºé—´è¿ç®—
    alpha=0.5,
    beta=0,
    device='cuda'
)
model = model.to('cuda')

# å‡†å¤‡åŒºé—´å€¼è¾“å…¥ [å·¦ç«¯ç‚¹ä»¬, å³ç«¯ç‚¹ä»¬]
# å½¢çŠ¶: (batch_size, 2 * n_features)
x = torch.rand(32, 14).to('cuda')  # 32 ä¸ªæ ·æœ¬ï¼Œ7 ä¸ªç‰¹å¾çš„åŒºé—´å€¼

# å‰å‘ä¼ æ’­
pred_left, pred_right = model(x)
print(f"é¢„æµ‹åŒºé—´å½¢çŠ¶: {pred_left.shape}, {pred_right.shape}")  # (32, 1), (32, 1)
```

### è®­ç»ƒç¤ºä¾‹ (æ”¹è¿›ç‰ˆé…ç½®)

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from IVIE.ivie import IE
from IVIE.iv_loss import ImprovedIntervalLoss

# å‡†å¤‡æ•°æ®
X_train = torch.rand(100, 14)  # 100 ä¸ªæ ·æœ¬ï¼Œ7 ä¸ªç‰¹å¾çš„åŒºé—´å€¼
y_train = torch.rand(100, 2)   # åŒºé—´å€¼æ ‡ç­¾ [ä¸‹ç•Œ, ä¸Šç•Œ]

train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)
test_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32)

# åˆ›å»ºæ¨¡å‹ (æ¨èé…ç½®)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = IE(
    feature_size=7,
    additivity_order=2,         # é™åˆ¶äº¤äº’é˜¶æ•°
    op='Algebraic_interval',
    device=device
).to(device)

# ä½¿ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°
criterion = ImprovedIntervalLoss(validity_weight=0.1, width_weight=0.05)

# ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-5)

# å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)

# è®­ç»ƒå¾ªç¯ (å¸¦æ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦)
best_val_loss = float('inf')
patience = 30
patience_counter = 0

for epoch in range(300):
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    train_loss = 0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        pred_l, pred_u = model(images)
        loss, _ = criterion(pred_l, pred_u, labels)
        train_loss += loss.item() * len(labels)
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
    
    avg_train_loss = train_loss / len(train_loader.dataset)
    
    # éªŒè¯é˜¶æ®µ
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            pred_l, pred_u = model(images)
            loss, _ = criterion(pred_l, pred_u, labels)
            val_loss += loss.item() * len(labels)
    
    avg_val_loss = val_loss / len(test_loader.dataset)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step()
    
    # æ—©åœæ£€æŸ¥
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        patience_counter = 0
        best_model_state = model.state_dict().copy()
    else:
        patience_counter += 1
    
    if patience_counter >= patience:
        print(f"æ—©åœè§¦å‘äº epoch {epoch + 1}")
        model.load_state_dict(best_model_state)
        break
    
    if (epoch + 1) % 20 == 0:
        print(f'Epoch [{epoch+1}/300], train_loss: {avg_train_loss:.6f}, '
              f'val_loss: {avg_val_loss:.6f}, lr: {optimizer.param_groups[0]["lr"]:.6f}')

print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
```

### å¿«é€Ÿè®­ç»ƒ (ä½¿ç”¨å†…ç½®æ–¹æ³•)

```python
# ä½¿ç”¨æ¨¡å‹å†…ç½®çš„è®­ç»ƒæ–¹æ³•
from IVIE.iv_loss import interval_loss as IntervalLoss

criterion = IntervalLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

model.fit_and_valid(
    train_Loader=train_loader,
    test_Loader=test_loader,
    criterion=criterion,
    optimizer=optimizer,
    device=device,
    epochs=100
)
```

### æ•°æ®é¢„å¤„ç†å»ºè®®

å¯¹äºUCIæ•°æ®é›†ç­‰å®é™…åº”ç”¨ï¼Œæ¨èä½¿ç”¨ä»¥ä¸‹é¢„å¤„ç†æ–¹å¼æ„é€ åŒºé—´å€¼ï¼š

```python
import pandas as pd
import numpy as np

# å‡è®¾ df æ˜¯å½’ä¸€åŒ–åçš„ç‰¹å¾æ•°æ® (å€¼åœ¨ [0, 1])
spread_ratio = 0.1  # ä½¿ç”¨10%çš„åŒºé—´å®½åº¦

# æ„é€ åŒºé—´å€¼
data_low = (df * (1 - spread_ratio)).clip(lower=0)
data_up = (df * (1 + spread_ratio)).clip(upper=1)

# âŒ ä¸æ¨èï¼šä½¿ç”¨æ ‡å‡†å·®å¯èƒ½äº§ç”Ÿè´Ÿå€¼
# data_low = df - 2 * df.std()  # å¯èƒ½ < 0
# data_up = df + 2 * df.std()   # å¯èƒ½ > 1
```

---

## å¸¸è§é—®é¢˜ä¸æœ€ä½³å®è·µ

### â“ ä¸ºä»€ä¹ˆé¢„æµ‹å€¼å…¨æ˜¯0æˆ–æ¥è¿‘0ï¼Ÿ

**åŸå› **: ä½¿ç”¨ `Algebraic_interval` ä¸”æœªé™åˆ¶ `additivity_order` æ—¶ï¼Œé«˜é˜¶ä¹˜æ³•å¯¼è‡´æ•°å€¼ä¸‹æº¢ã€‚

**è§£å†³æ–¹æ¡ˆ**:
```python
# âœ… è®¾ç½® additivity_order
model = IE(feature_size=7, additivity_order=2, op='Algebraic_interval')
```

### â“ å¦‚ä½•é€‰æ‹© additivity_orderï¼Ÿ

| additivity_order | ç‰¹å¾ç»„åˆæ•° (n=7) | é€‚ç”¨åœºæ™¯ |
|-----------------|-----------------|---------|
| 1 | 7 | ä»…è€ƒè™‘å•ä¸ªç‰¹å¾ï¼Œçº¿æ€§æ¨¡å‹ |
| 2 | 28 | **æ¨è**ï¼Œè€ƒè™‘ç‰¹å¾å¯¹äº¤äº’ |
| 3 | 63 | è€ƒè™‘ä¸‰å…ƒäº¤äº’ï¼Œè®¡ç®—é‡é€‚ä¸­ |
| 7 (å…¨éƒ¨) | 127 | æ‰€æœ‰äº¤äº’ï¼Œå¯èƒ½æ•°å€¼ä¸‹æº¢ |

**æ¨è**: ä» 2 å¼€å§‹ï¼Œæ ¹æ®éªŒè¯é›†æ€§èƒ½è°ƒæ•´åˆ° 3 æˆ– 4ã€‚

### â“ é€‰æ‹© Algebraic_interval è¿˜æ˜¯ Min_intervalï¼Ÿ

| æ“ä½œç±»å‹ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|---------|
| **Algebraic_interval** | è¿ç»­å¯å¯¼ï¼Œä¾¿äºä¼˜åŒ– | éœ€è¦é™åˆ¶é˜¶æ•°é¿å…ä¸‹æº¢ | å½’ä¸€åŒ–æ•°æ®ï¼Œéœ€è¦ç‰¹å¾äº¤äº’ |
| **Min_interval** | æ•°å€¼ç¨³å®šï¼Œæ— ä¸‹æº¢é—®é¢˜ | éå…‰æ»‘ï¼Œä¼˜åŒ–å¯èƒ½è¾ƒæ…¢ | åŸå§‹æ•°æ®ï¼Œç¨³å¥æ€§è¦æ±‚é«˜ |

**å»ºè®®**: ä¼˜å…ˆå°è¯• `Algebraic_interval` + `additivity_order=2`

### â“ æŸå¤±å‡½æ•°å¦‚ä½•é€‰æ‹©ï¼Ÿ

| æŸå¤±å‡½æ•° | é€‚ç”¨åœºæ™¯ | æƒé‡å»ºè®® |
|---------|---------|---------|
| `interval_loss` | åŸºå‡†æµ‹è¯•ï¼Œç®€å•åœºæ™¯ | - |
| `ImprovedIntervalLoss` | **æ¨è**ï¼Œå¤§å¤šæ•°å®é™…åº”ç”¨ | validity_weight=0.1, width_weight=0.05 |
| `HausdorffIntervalLoss` | å¯¹åŒºé—´ç«¯ç‚¹è¯¯å·®æ•æ„Ÿçš„åœºæ™¯ | validity_weight=0.1 |

### â“ å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨å¦‚ä½•è®¾ç½®ï¼Ÿ

**æ¨èé…ç½®**:
```python
# AdamW + ä½™å¼¦é€€ç«
optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-5)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)

# æˆ– Adam + ReduceLROnPlateau
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)
```

**å­¦ä¹ ç‡å»ºè®®**:
- åˆå§‹å­¦ä¹ ç‡: 0.001 - 0.01
- ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨é€æ­¥è¡°å‡
- æ·»åŠ æ¢¯åº¦è£å‰ª: `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)`

### â“ å¦‚ä½•å¤„ç†æ•°æ®é¢„å¤„ç†ï¼Ÿ

**åŒºé—´æ„é€ æ–¹å¼å¯¹æ¯”**:

```python
# âŒ ä¸æ¨èï¼šæ ‡å‡†å·®æ–¹æ³• (å¯èƒ½äº§ç”Ÿè´Ÿå€¼æˆ–è¶Šç•Œ)
data_low = df - 2 * df.std()  # å¯èƒ½ < 0
data_up = df + 2 * df.std()   # å¯èƒ½ > 1

# âœ… æ¨èï¼šæ¯”ä¾‹åç§» (ä¿è¯èŒƒå›´)
spread_ratio = 0.1
data_low = (df * (1 - spread_ratio)).clip(lower=0)
data_up = (df * (1 + spread_ratio)).clip(upper=1)

# âœ… æ¨èï¼šç»å¯¹åç§» + è£å‰ª
epsilon = 0.1
data_low = (df - epsilon).clip(lower=0)
data_up = (df + epsilon).clip(upper=1)
```

---

## æ€§èƒ½è°ƒä¼˜å»ºè®®

### 1. è®­ç»ƒç­–ç•¥

âœ… **ä½¿ç”¨æ—©åœ**:
```python
patience = 30
best_val_loss = float('inf')
patience_counter = 0

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
if avg_val_loss < best_val_loss:
    best_val_loss = avg_val_loss
    patience_counter = 0
    torch.save(model.state_dict(), 'best_model.pth')
else:
    patience_counter += 1
    if patience_counter >= patience:
        break
```

âœ… **æ‰¹é‡å¤§å°**:
- å°æ•°æ®é›† (< 1000): batch_size = 16-32
- ä¸­ç­‰æ•°æ®é›† (1000-10000): batch_size = 32-64
- å¤§æ•°æ®é›† (> 10000): batch_size = 64-128

âœ… **è®­ç»ƒè½®æ•°**:
- é…åˆæ—©åœ: epochs = 300-500
- æ— æ—©åœ: epochs = 100-200

### 2. æ¨¡å‹åˆå§‹åŒ–

å½“å‰æ¨¡å‹ä½¿ç”¨å‡åŒ€åˆå§‹åŒ–ã€‚å¯ä»¥å°è¯•æ”¹è¿›:

```python
# åœ¨ IE.__init__ ä¸­ä¿®æ”¹
# é»˜è®¤: dummy = (1./self.columns_num) * torch.ones((self.nVars, 1))
# æ”¹è¿›: ä½¿ç”¨ Xavier åˆå§‹åŒ–
import torch.nn as nn
init_val = torch.empty((self.nVars, 1))
nn.init.xavier_uniform_(init_val)
init_val = torch.abs(init_val) * 0.5 + 0.1  # ä¿è¯æ­£å€¼
self.vars = torch.nn.Parameter(init_val)
```

### 3. ç›‘æ§æŒ‡æ ‡

å»ºè®®åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§:
- è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±
- åŒºé—´æœ‰æ•ˆæ€§ (é¢„æµ‹çš„ä¸‹ç•Œ â‰¤ ä¸Šç•Œçš„æ¯”ä¾‹)
- å¹³å‡ç»å¯¹è¯¯å·® (MAE): `torch.mean(torch.abs(pred_l - true_l) + torch.abs(pred_u - true_u))`
- åŒºé—´å®½åº¦: `torch.mean(pred_u - pred_l)`

---

## ç‰ˆæœ¬å†å²

### v1.1.0 (2026å¹´1æœˆ)
- âœ¨ æ–°å¢ `additivity_order` æ”¯æŒï¼Œè§£å†³é«˜é˜¶ç»„åˆçš„æ•°å€¼ä¸‹æº¢é—®é¢˜
- âœ¨ æ–°å¢ `ImprovedIntervalLoss` å’Œ `HausdorffIntervalLoss` æŸå¤±å‡½æ•°
- ğŸ”§ ä¼˜åŒ– `forward` æ–¹æ³•ï¼Œæ­£ç¡®å¤„ç†å—é™é˜¶æ•°çš„ç‰¹å¾çŸ©é˜µ
- ğŸ”§ ä¼˜åŒ– `ivie_nn_vars` æ–¹æ³•ï¼Œæ”¯æŒå—é™é˜¶æ•°çš„æ¨¡ç³Šæµ‹åº¦æ„å»º
- ğŸ“ å®Œå–„æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹
- ğŸ¯ æå‡åœ¨UCIç­‰å®é™…æ•°æ®é›†ä¸Šçš„æ€§èƒ½

### v1.0.0 (åˆå§‹ç‰ˆæœ¬)
- åŸºç¡€IEç½‘ç»œå®ç°
- æ”¯æŒ Algebraic_interval å’Œ Min_interval æ“ä½œ
- åŸºç¡€æŸå¤±å‡½æ•° interval_loss

---

**æœ€åæ›´æ–°**: 2026å¹´1æœˆ14æ—¥
